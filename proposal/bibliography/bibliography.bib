@misc{dai_document_2015,
    title = {Document {Embedding} with {Paragraph} {Vectors}},
    url = {http://arxiv.org/abs/1507.07998},
    doi = {10.48550/arXiv.1507.07998},
    abstract = {Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.},
    urldate = {2023-11-25},
    publisher = {arXiv},
    author = {Dai, Andrew M. and Olah, Christopher and Le, Quoc V.},
    month = jul,
    year = {2015},
    note = {arXiv:1507.07998 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@book{de_api_2017,
    address = {Berkeley},
    edition = {First Edition},
    title = {{API} {Management}: an {Architect}'s {Guide} to {Developing} and {Managing} {APIs} for {Your} {Organization}},
    isbn = {978-1-4842-1306-3},
    shorttitle = {{API} {Management}},
    language = {eng},
    publisher = {Apress},
    author = {De, Brajesh},
    year = {2017},
}

@misc{cer_universal_2018,
    title = {Universal {Sentence} {Encoder}},
    url = {http://arxiv.org/abs/1803.11175},
    abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
    urldate = {2023-11-25},
    publisher = {arXiv},
    author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
    month = apr,
    year = {2018},
    note = {arXiv:1803.11175 [cs]},
    keywords = {Computer Science - Computation and Language},
}

@misc{swagger_openapi_2021,
    title = {{OpenAPI} {Specification} v3.1.0 {\textbar} {Introduction}, {Definitions}, \& {More}},
    url = {https://spec.openapis.org/oas/v3.1.0},
    urldate = {2023-11-25},
    author = {{Swagger}},
    month = feb,
    year = {2021},
}

@misc{guo_testing_2022,
    address = {Rochester, NY},
    type = {{SSRN} {Scholarly} {Paper}},
    title = {Testing and {Validating} the {Cosine} {Similarity} {Measure} for {Textual} {Analysis}},
    url = {https://papers.ssrn.com/abstract=4258463},
    doi = {10.2139/ssrn.4258463},
    abstract = {Textual similarity has drawn much attention in the recent literature of accounting and related fields. There has been, however, limited work to systematically test and validate its measures. In this paper I conduct three incremental studies to comprehensively test and validate the commonly used cosine similarity (COS) method. The results show that the 5-gram COS measure (based on phrases of five consecutive words) is a viable approach to assessing textual similarity. In addition, I develop a new scale termed boundary n-gram (the longest match that can be found in two texts) to obtain complementary information about textual similarity.},
    language = {en},
    urldate = {2023-11-25},
    author = {Guo, Ken},
    month = oct,
    year = {2022},
    keywords = {Construct Validation, Cosine Similarity, n-gram, Textual Research Method, Textual Similarity},
}

@article{cover_nearest_1967,
    title = {Nearest {Neighbor} {Pattern} {Classification}},
    volume = {13},
    issn = {1557-9654},
    url = {https://ieeexplore.ieee.org/document/1053964},
    doi = {10.1109/TIT.1967.1053964},
    abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of errorRof such a rule must be at least as great as the Bayes probability of errorR{\textasciicircum}{\textbackslash}ast–the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in theM-category case thatR{\textasciicircum}{\textbackslash}ast {\textbackslash}leq R {\textbackslash}leq R{\textasciicircum}{\textbackslash}ast(2 –MR{\textasciicircum}{\textbackslash}ast/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
    number = {1},
    urldate = {2023-11-25},
    journal = {IEEE Transactions on Information Theory},
    author = {Cover, T. and Hart, P.},
    month = jan,
    year = {1967},
    note = {Conference Name: IEEE Transactions on Information Theory},
    pages = {21--27},
}

@article{maaten_visualizing_2008,
    title = {Visualizing {Data} using t-{SNE}},
    volume = {9},
    issn = {1533-7928},
    url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
    abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
    number = {86},
    urldate = {2023-11-25},
    journal = {Journal of Machine Learning Research},
    author = {Maaten, Laurens van der and Hinton, Geoffrey},
    year = {2008},
    pages = {2579--2605},
}

@inproceedings{hinton_stochastic_2002,
    title = {Stochastic {Neighbor} {Embedding}},
    volume = {15},
    url = {https://papers.nips.cc/paper_files/paper/2002/hash/6150ccc6069bea6b5716254057a194ef-Abstract.html},
    abstract = {We describe a probabilistic approach to the task of placing objects, de- scribed by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to deﬁne a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribu- tion as well as possible when the same operation is performed on the low-dimensional “images” of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional im- ages. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word “bank”, to have versions close to the images of both “river” and “ﬁnance” without forcing the images of outdoor concepts to be located close to those of corporate concepts.},
    urldate = {2023-11-25},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
    publisher = {MIT Press},
    author = {Hinton, Geoffrey E and Roweis, Sam},
    year = {2002},
}

@inproceedings{frakes_information_1992,
    title = {Information {Retrieval}: {Data} {Structures} and {Algorithms}},
    shorttitle = {Information {Retrieval}},
    url = {https://www.semanticscholar.org/paper/Information-Retrieval%3A-Data-Structures-and-Frakes-Baeza-Yates/3f133e39f3fb543f25a1a75400b81c0d42c6a91c},
    abstract = {An edited volume containing data structures and algorithms for information retrieved including a disk with examples written in C. For programmers and students interested in parsing text, automated indexing, its the first collection in book form of the basic data structures and algorithms that are critical to the storage and retrieval of documents.},
    urldate = {2023-11-26},
    author = {Frakes, W. and Baeza-Yates, R.},
    month = jun,
    year = {1992},
}
