@misc{dai_document_2015,
	title = {Document {Embedding} with {Paragraph} {Vectors}},
	url = {http://arxiv.org/abs/1507.07998},
	doi = {10.48550/arXiv.1507.07998},
	abstract = {Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.},
	urldate = {2023-11-25},
	publisher = {arXiv},
	author = {Dai, Andrew M. and Olah, Christopher and Le, Quoc V.},
	month = jul,
	year = {2015},
	note = {arXiv:1507.07998 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@book{de_api_2017,
	address = {Berkeley},
	edition = {First Edition},
	title = {{API} {Management}: an {Architect}'s {Guide} to {Developing} and {Managing} {APIs} for {Your} {Organization}},
	isbn = {978-1-4842-1306-3},
	shorttitle = {{API} {Management}},
	language = {eng},
	publisher = {Apress},
	author = {De, Brajesh},
	year = {2017},
}

@misc{cer_universal_2018,
	title = {Universal {Sentence} {Encoder}},
	url = {http://arxiv.org/abs/1803.11175},
	abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
	urldate = {2023-11-25},
	publisher = {arXiv},
	author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
	month = apr,
	year = {2018},
	note = {arXiv:1803.11175 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{swagger_openapi_2021,
	title = {{OpenAPI} {Specification} v3.1.0 {\textbar} {Introduction}, {Definitions}, \& {More}},
	url = {https://spec.openapis.org/oas/v3.1.0},
	urldate = {2023-11-25},
	author = {{Swagger}},
	month = feb,
	year = {2021},
}

@misc{guo_testing_2022,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Testing and {Validating} the {Cosine} {Similarity} {Measure} for {Textual} {Analysis}},
	url = {https://papers.ssrn.com/abstract=4258463},
	doi = {10.2139/ssrn.4258463},
	abstract = {Textual similarity has drawn much attention in the recent literature of accounting and related fields. There has been, however, limited work to systematically test and validate its measures. In this paper I conduct three incremental studies to comprehensively test and validate the commonly used cosine similarity (COS) method. The results show that the 5-gram COS measure (based on phrases of five consecutive words) is a viable approach to assessing textual similarity. In addition, I develop a new scale termed boundary n-gram (the longest match that can be found in two texts) to obtain complementary information about textual similarity.},
	language = {en},
	urldate = {2023-11-25},
	author = {Guo, Ken},
	month = oct,
	year = {2022},
	keywords = {Construct Validation, Cosine Similarity, n-gram, Textual Research Method, Textual Similarity},
}

@article{cover_nearest_1967,
	title = {Nearest {Neighbor} {Pattern} {Classification}},
	volume = {13},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/document/1053964},
	doi = {10.1109/TIT.1967.1053964},
	abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of errorRof such a rule must be at least as great as the Bayes probability of errorR{\textasciicircum}{\textbackslash}ast–the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in theM-category case thatR{\textasciicircum}{\textbackslash}ast {\textbackslash}leq R {\textbackslash}leq R{\textasciicircum}{\textbackslash}ast(2 –MR{\textasciicircum}{\textbackslash}ast/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
	number = {1},
	urldate = {2023-11-25},
	journal = {IEEE Transactions on Information Theory},
	author = {Cover, T. and Hart, P.},
	month = jan,
	year = {1967},
	note = {Conference Name: IEEE Transactions on Information Theory},
	pages = {21--27},
}

@article{maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
	abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
	number = {86},
	urldate = {2023-11-25},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605},
}

@inproceedings{hinton_stochastic_2002,
	title = {Stochastic {Neighbor} {Embedding}},
	volume = {15},
	url = {https://papers.nips.cc/paper_files/paper/2002/hash/6150ccc6069bea6b5716254057a194ef-Abstract.html},
	abstract = {We describe a probabilistic approach to the task of placing objects, de- scribed by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to deﬁne a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribu- tion as well as possible when the same operation is performed on the low-dimensional “images” of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional im- ages. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word “bank”, to have versions close to the images of both “river” and “ﬁnance” without forcing the images of outdoor concepts to be located close to those of corporate concepts.},
	urldate = {2023-11-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Hinton, Geoffrey E and Roweis, Sam},
	year = {2002},
}

@inproceedings{frakes_information_1992,
	title = {Information {Retrieval}: {Data} {Structures} and {Algorithms}},
	shorttitle = {Information {Retrieval}},
	url = {https://www.semanticscholar.org/paper/Information-Retrieval%3A-Data-Structures-and-Frakes-Baeza-Yates/3f133e39f3fb543f25a1a75400b81c0d42c6a91c},
	abstract = {An edited volume containing data structures and algorithms for information retrieved including a disk with examples written in C. For programmers and students interested in parsing text, automated indexing, its the first collection in book form of the basic data structures and algorithms that are critical to the storage and retrieval of documents.},
	urldate = {2023-11-26},
	author = {Frakes, W. and Baeza-Yates, R.},
	month = jun,
	year = {1992},
}

@article{peltonen_information_2015,
	title = {Information retrieval approach to meta-visualization},
	volume = {99},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-014-5464-x},
	doi = {10.1007/s10994-014-5464-x},
	abstract = {Visualization is crucial in the first steps of data analysis. In visual data exploration with scatter plots, no single plot is sufficient to analyze complicated high-dimensional data sets. Given numerous visualizations created with different features or methods, meta-visualization is needed to analyze the visualizations together. We solve how to arrange numerous visualizations onto a meta-visualization display, so that their similarities and differences can be analyzed. Visualization has recently been formalized as an information retrieval task; we extend this approach, and formalize meta-visualization as an information retrieval task whose performance can be rigorously quantified and optimized. We introduce a machine learning approach to optimize the meta-visualization, based on an information retrieval perspective: two visualizations are similar if the analyst would retrieve similar neighborhoods between data samples from either visualization. Based on the approach, we introduce a nonlinear embedding method for meta-visualization: it optimizes locations of visualizations on a display, so that visualizations giving similar information about data are close to each other. In experiments we show such meta-visualization outperforms alternatives, and yields insight into data in several case studies.},
	language = {en},
	number = {2},
	urldate = {2023-11-28},
	journal = {Machine Learning},
	author = {Peltonen, Jaakko and Lin, Ziyuan},
	month = may,
	year = {2015},
	keywords = {Meta-visualization, Neighbor embedding, Nonlinear dimensionality reduction},
	pages = {189--229},
}

@article{ma_restful_2023,
	title = {{RESTful} {API} {Analysis}, {Recommendation}, and {Client} {Code} {Retrieval}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/12/5/1252},
	doi = {10.3390/electronics12051252},
	abstract = {Numerous companies create innovative software systems using Web APIs (Application Programming Interfaces). API search engines and API directory services, such as ProgrammableWeb, Rapid API Hub, APIs.guru, and API Harmony, have been developed to facilitate the utilization of various APIs. Unfortunately, most API systems provide only superficial support, with no assistance in obtaining relevant APIs or examples of code usage. To better realize the “FAIR” (Findability, Accessibility, Interoperability, and Reusability) features for the usage of Web APIs, in this study, we developed an API inspection system (referred to as API Prober) to provide a new API directory service with multiple supplemental functionalities. To facilitate the findability and accessibility of APIs, API Prober transforms OAS (OpenAPI Specifications) into a graph structure and automatically annotates the semantic concepts using LDA (Latent Dirichlet Allocation) and WordNet. To enhance interoperability, API Prober also classifies APIs by clustering OAS documents and recommends alternative services to be substituted or merged with the target service. Finally, to support reusability, API Prober makes it possible to retrieve examples of API utilization code in Java by parsing source code in GitHub. The experimental results demonstrate the effectiveness of the API Prober in recommending relevant services and providing usage examples based on real-world client code. This research contributes to providing viable methods to appropriately analyze and cluster Web APIs, and recommend APIs and client code examples.},
	language = {en},
	number = {5},
	urldate = {2023-12-01},
	journal = {Electronics},
	author = {Ma, Shang-Pin and Hsu, Ming-Jen and Chen, Hsiao-Jung and Lin, Chuan-Jie},
	month = jan,
	year = {2023},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {cluster analysis, code example, GitHub, Latent Dirichlet Allocation, OpenAPI Specification, service recommendation},
	pages = {1252},
}

@misc{moon_api-miner_2023,
	title = {{API}-{Miner}: an {API}-to-{API} {Specification} {Recommendation} {Engine}},
	shorttitle = {{API}-{Miner}},
	url = {http://arxiv.org/abs/2212.07253},
	abstract = {When designing a new API for a large project, developers need to make smart design choices so that their code base can grow sustainably. To ensure that new API components are well designed, developers can learn from existing API components. However, the lack of standardized methods for comparing API designs makes this learning process time-consuming and difficult. To address this gap we developed API-Miner, to the best of our knowledge, one of the first API-to-API specification recommendation engines. API-Miner retrieves relevant specification components written in OpenAPI (a widely adopted language used to describe web APIs). API-miner presents several significant contributions, including: (1) novel methods of processing and extracting key information from OpenAPI specifications, (2) innovative feature extraction techniques that are optimized for the highly technical API specification domain, and (3) a novel log-linear probabilistic model that combines multiple signals to retrieve relevant and high quality OpenAPI specification components given a query specification. We evaluate API-Miner in both quantitative and qualitative tasks and achieve an overall of 91.7\% recall@1 and 56.2\% F1, which surpasses baseline performance by 15.4\% in recall@1 and 3.2\% in F1. Overall, API-Miner will allow developers to retrieve relevant OpenAPI specification components from a public or internal database in the early stages of the API development cycle, so that they can learn from existing established examples and potentially identify redundancies in their work. It provides the guidance developers need to accelerate development process and contribute thoughtfully designed APIs that promote code maintainability and quality. Code is available on GitHub at https://github.com/jpmorganchase/api-miner.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Moon, Sae Young and Kerr, Gregor and Silavong, Fran and Moran, Sean},
	month = jul,
	year = {2023},
	note = {arXiv:2212.07253 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings{ma_api_2020,
	address = {Cham},
	series = {Lecture {Notes} on {Data} {Engineering} and {Communications} {Technologies}},
	title = {{API} {Prober} – {A} {Tool} for {Analyzing} {Web} {API} {Features} and {Clustering} {Web} {APIs}},
	isbn = {978-3-030-34986-8},
	doi = {10.1007/978-3-030-34986-8_6},
	abstract = {Nowadays, Web services attract more and more attentions. Many companies expose their data or services by publishing Web APIs (Application Programming Interface) to let users create innovative services or applications. To ease the use of various and complex APIs, multiple API directory services or API search engines, such as Mashape, API Harmony, and ProgrammableWeb, are emerging in recent years. However, most API systems are only able to help developers to understand Web APIs. Furthermore, these systems do neither provide usage examples for users, nor help users understand the “closeness” between APIs. Therefore, we propose a system, referred to as API Prober, to address the above issues by constructing an API “dictionary”. There are multiple main features of API Prober. First, API Prober transforms OAS (OpenAPI Specification 2.0) into the graph structure in Neo4J database and annotates the semantic concepts on each graph node by using LDA (Latent Dirichlet Allocation) and WordNet. Second, by parsing source codes in the GitHub, API Prober is able to retrieve code examples that utilize APIs. Third, API Prober performs API classification through cluster analysis for OAS documents. Finally, the experimental results show that API Prober can appropriately produce service clusters.},
	language = {en},
	booktitle = {Advances in {E}-{Business} {Engineering} for {Ubiquitous} {Computing}},
	publisher = {Springer International Publishing},
	author = {Ma, Shang-Pin and Hsu, Ming-Jen and Chen, Hsiao-Jung and Su, Yu-Sheng},
	editor = {Chao, Kuo-Ming and Jiang, Lihong and Hussain, Omar Khadeer and Ma, Shang-Pin and Fei, Xiang},
	year = {2020},
	keywords = {Cluster analysis, GitHub, Semantic annotation, Web API analysis},
	pages = {81--96},
}

@inproceedings{kim_empirical_2019,
	title = {An {Empirical} {Analysis} of {GraphQL} {API} {Schemas} in {Open} {Code} {Repositories} and {Package} {Registries}},
	url = {https://www.semanticscholar.org/paper/An-Empirical-Analysis-of-GraphQL-API-Schemas-in-and-Kim-Consens/55985b1efb3ac3c8951c3e64337d5b743cd9cb1e},
	abstract = {GraphQL is a query language for APIs that has been increasingly adopted by Web developers since its specification was open sourced in 2015. The GraphQL framework lets API clients tailor data requests by using queries that return JSON objects described using GraphQL Schema. We present initial results of an exploratory empirical study with the goal of characterizing GraphQL Schemas in open code repositories and package registries. Our first approach identifies over 20 thousand GraphQL-related projects in publicly accessible repositories hosted by GitHub. Our second, and complementary, approach uses package registries to find over 37 thousand dependent packages and repositories. In addition, over 2 thousand schema files were loaded into the GraphQL-JS reference implementation to conduct a detailed analysis of the schema information. Our study provides insights into the usage of different schema constructs, the number of distinct types and the most popular types in schemas, as well as the presence of cycles in schemas. 1 Motivation and Approach The schema of a GraphQL API describes the data and the types of queries supported by the API. An empirical study of the GraphQL schemas used by open source projects, therefore, provides useful information about the characteristics of data interfaces. Currently, there is no comprehensive collection of such schemas or a tool that helps gather schemas from GraphQL APIs. The goal of the work presented in this paper is i) to establish a method to extract schemas into a single collection for analyses and ii) to conduct an empirical analysis of the schemas. 1.1 Data Collection Method APIs-guru has the most comprehensive list of public GraphQL APIs with links to endpoints and their documentation. By using APIs-guru, combined with manual effort through keyword searching, we collected 67 schemas of distinct APIs. Authentication requirements for most publicly available APIs hindered the efficiency and possible automation of schema extraction. Hence, we decided to take a different approach by extracting schemas from open source repositories from GitHub and used three sources to identify GraphQL repositories.},
	urldate = {2023-12-04},
	author = {Kim, Yun Wan and Consens, M. and Hartig, O.},
	year = {2019},
}

@inproceedings{yang_towards_2018,
	address = {New York, NY, USA},
	series = {{MSR} '18},
	title = {Towards extracting web {API} specifications from documentation},
	isbn = {978-1-4503-5716-6},
	url = {https://doi.org/10.1145/3196398.3196411},
	doi = {10.1145/3196398.3196411},
	abstract = {Web API specifications are machine-readable descriptions of APIs. These specifications, in combination with related tooling, simplify and support the consumption of APIs. However, despite the increased distribution of web APIs, specifications are rare and their creation and maintenance heavily rely on manual efforts by third parties. In this paper, we propose an automatic approach and an associated tool called D2Spec for extracting significant parts of such specifications from web API documentation pages. Given a seed online documentation page of an API, D2Spec first crawls all documentation pages on the API, and then uses a set of machine-learning techniques to extract the base URL, path templates, and HTTP methods - collectively describing the endpoints of the API. We evaluate whether D2Spec can accurately extract endpoints from documentation on 116 web APIs. The results show that D2Spec achieves a precision of 87.1\% in identifying base URLs, a precision of 80.3\% and a recall of 80.9\% in generating path templates, and a precision of 83.8\% and a recall of 77.2\% in extracting HTTP methods. In addition, in an evaluation on 64 APIs with pre-existing API specifications, D2Spec revealed many inconsistencies between web API documentation and their corresponding publicly available specifications. API consumers would benefit from D2Spec pointing them to, and allowing them thus to fix, such inconsistencies.},
	urldate = {2023-12-04},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Jinqiu and Wittern, Erik and Ying, Annie T. T. and Dolby, Julian and Tan, Lin},
	month = may,
	year = {2018},
	pages = {454--464},
}

@misc{tsai_rest_2021,
	title = {{REST} {API} {Fuzzing} by {Coverage} {Level} {Guided} {Blackbox} {Testing}},
	url = {http://arxiv.org/abs/2112.15485},
	doi = {10.48550/arXiv.2112.15485},
	abstract = {With the growth of web applications, REST APIs have become the primary communication method between services. In order to ensure system reliability and security, software quality can be assured by effective testing methods. Black box fuzz testing is one of the effective methods to perform tests on a large scale. However, conventional black box fuzz testing generates random data without judging the quality of the input. We implement a black box fuzz testing method for REST APIs. It resolves the issues of blind mutations without knowing the effectiveness by Test Coverage Level feedback. We also enhance the mutation strategies by reducing the testing complexity for REST APIs, generating more appropriate test cases to cover possible paths. We evaluate our method by testing two large open-source projects and 89 bugs are reported and confirmed. In addition, we find 351 bugs from 64 remote API services in APIs.guru. The work is in https://github.com/iasthc/hsuan-fuzz.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Tsai, Chung-Hsuan and Tsai, Shi-Chun and Huang, Shih-Kun},
	month = dec,
	year = {2021},
	note = {arXiv:2112.15485 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{yasmin_first_2020,
	title = {A {First} {Look} at the {Deprecation} of {RESTful} {APIs}: {An} {Empirical} {Study}},
	shorttitle = {A {First} {Look} at the {Deprecation} of {RESTful} {APIs}},
	url = {http://arxiv.org/abs/2008.12808},
	doi = {10.48550/arXiv.2008.12808},
	abstract = {REpresentational State Transfer (REST) is considered as one standard software architectural style to build web APIs that can integrate software systems over the internet. However, while connecting systems, RESTful APIs might also break the dependent applications that rely on their services when they introduce breaking changes, e.g., an older version of the API is no longer supported. To warn developers promptly and thus prevent critical impact on downstream applications, a deprecated-removed model should be followed, and deprecation-related information such as alternative approaches should also be listed. While API deprecation analysis as a theme is not new, most existing work focuses on non-web APIs, such as the ones provided by Java and Android. To investigate RESTful API deprecation, we propose a framework called RADA (RESTful API Deprecation Analyzer). RADA is capable of automatically identifying deprecated API elements and analyzing impacted operations from an OpenAPI specification, a machine-readable profile for describing RESTful web service. We apply RADA on 2,224 OpenAPI specifications of 1,368 RESTful APIs collected from APIs.guru, the largest directory of OpenAPI specifications. Based on the data mined by RADA, we perform an empirical study to investigate how the deprecated-removed protocol is followed in RESTful APIs and characterize practices in RESTful API deprecation. The results of our study reveal several severe deprecation-related problems in existing RESTful APIs. Our implementation of RADA and detailed empirical results are publicly available for future intelligent tools that could automatically identify and migrate usage of deprecated RESTful API operations in client code.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Yasmin, Jerin and Tian, Yuan and Yang, Jinqiu},
	month = aug,
	year = {2020},
	note = {arXiv:2008.12808 [cs]},
	keywords = {Computer Science - Software Engineering},
}
