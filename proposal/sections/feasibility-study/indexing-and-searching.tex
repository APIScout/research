With the API Scout platform being a data aggregator for OpenAPI Specifications (section~\ref{subsec:openapi-specification}), we need a way of efficiently indexing and searching through the plethora of crawled documents in our database.
When dealing with OpenAPI Specifications, we are working with JSON or YAML files (which are structured).
Moreover, these files follow the OpenAPI Specifications; thus we know that the documents will contain a very similar structure. \\ \\
Since we are dealing with a very high number of documents, it is paramount to have a fast and reliable index that can be searched through.
For this reason, we decided to use vector embeddings to represent documents in our database.
Embedding models offer a way of transforming documents into vectors of numbers, and -- in our case -- we used Google's Universal Sentence Encoder~\cite{cer2018universal} to perform the aforementioned task.
The embedding is done on only a part of the document's content, namely everything that is contained in the \verb|description|, \verb|name|, \verb|title|, and \verb|summary| labels.
We chose to do so because these labels are the only ones that contain natural language descriptions of the api in general or of api endpoints.
\begin{figure}
    \centering
    \includegraphics[width=13cm]{../out/plots/SNE}
    \caption{T-SNE Plot}
    \label{fig:tSNE-plot}
\end{figure}
After the creation of the embeddings, we saved the vectors -- as well as the name, version, and id -- of the specifications in an ElasticSearch database.
We chose ElasticSearch and the ELK (Elastic-Logstash-Kibana) stack because it is very rapid and scalable.
Moreover, ElasticSearch offers the possibility of searching documents based on the \textit{K}-NN algorithm.
This solution seems to be the most promising for the amount of data and hardware availability we have for this project.
We were considering also deep-learning models but discarded them since they are resource intensive and would be overkill for this kind of application. \\ \\
The results obtained with the embedding + K-NN solution are very promising, as we can see in Figure~\ref{fig:tSNE-plot}.
With the \("\)Y\("\) markers representing the queries, and the dots representing the most similar documents found for that query, we can see that the dots and \("\)Y\("\) markers of the same colors are fairly close to each other.
Moreover, we performed a validation step with a ground truth.
From the validation step, we concluded that the precision of the document retrieval is $p = 0.6$, while the recall is $r = 1.0$.
This validation was done on 10 different queries.
The position in which each ground truth appeared in the searches can be found below.
\begin{verbatim}
  Position in which the ground truth was found:

      "NFL v3 Ply-by-Play" was found at position #1
      "AWS IoT Secure Tunneling" was found at position #4
      "Transport Department" was found at position #4
      "Text Analytics & Sentiment Analysis API | api.text2data.com" was
        found at position #4
      "MailboxValidator Free Email Checker" was found at position #1
      "Interzoid Country Data Standardization API" was found at position #1
      "Soccer v3 Projections" was found at position #1
      "PolicyClient" was found at position #1
      "NetworkManagementClient" was found at position #3
      "Interzoid Get Weather City API" was found at position #2
\end{verbatim}
As we can see, all ground truths we found in the top 5 results -- hence $p = 1.0$.
The validation on the ground truth, though, does not paint the full picture of the capabilities this solution offers.
For example, if we try searching using the query: \("\)american sports news\("\), the top 5 documents returned by the information retrieval system will be the following.
\begin{verbatim}
  These are the top 5 results of the query "american sports news":

      1. NFL v3 Play-by-Play     v.1.0   [67%]
      2. News Plugin             v.1     [66%]
      3. Soccer v3 Projections   v.1.0   [64%]
      4. MLB v3 Projections      v.1.0   [64%]
      5. NFL v3 Scores           v.1.0   [64%]
\end{verbatim}
As we can see in the results above, the engine is able to recognize that both the \textit{NFL} and the \textit{MLB} are professional American sport leagues.
The former being the National Football League, and the latter being the Major League Baseball.
Moreover, it is also able to understand that soccer is a sport and return it. \\ \\
As can be seen both in the precision ($p = 0.6$), and the above example, there are still some noisy results that match part or none of the query, but this is acceptable.
The reason is that this is not the only means of searching.
In tandem with the natural language query, there is also a DSL that can be used to filter the documents and obtain a more accurate result.
